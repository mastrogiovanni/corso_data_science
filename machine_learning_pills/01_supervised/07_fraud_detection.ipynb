{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_fraud_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMJYpGweSnpsZ1ZWSyMLD5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickprock/corso_data_science/blob/devs/machine_learning_pills/01_supervised/07_fraud_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvBbFKxEPuMW",
        "colab_type": "text"
      },
      "source": [
        "## Credit Card Fraud Detection\n",
        "\n",
        "### Dataset Sbilanciati\n",
        "\n",
        "<br>\n",
        "\n",
        "![fraud](https://www.techexplorist.com/wp-content/uploads/2018/09/MIT-Fraud-Detection.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://www.techexplorist.com/false-positive-reduction-credit-card-fraud-detection/17191/)\n",
        "\n",
        "<br>\n",
        "\n",
        "### Obiettivo del notebook\n",
        "\n",
        "L'obiettivo di questo notebook è proprio la scoperta di frodi su carte di credito in un dataset molto sbilanciato.\n",
        "\n",
        "Quando si tratta di individuare frodi, guasti o comunque eventi rari i nostri dataset saranno probabilmente molto sbilanciati, ovvero una classe sarà molto più rappresentata delle altre.\n",
        "\n",
        "In questo caso il nostro stimatore potrebbe non individuare nessun pattern e classificare tutte le osservazioni nella medesima classe. Per ovviare a questo problema si possono usare le segiuenti tecniche:\n",
        "* **undersampling**: eliminare molte osservazioni della classe più rappresentata in modo da bilanciare il dataset. Consigliato quando si hanno molte osservazioni.\n",
        "\n",
        "* **oversampling**: duplicare le osservazioni della classe con eventi rari per bilanciare il dataset. Io personalmente non consiglio questa strategia, si introduce molto rumore che potrebbe portare ad overfitting, se proprio non si può applicare l'undersampling il mio consiglio è di fare oversampling *mitigando* lo sbilancimento, ad esempio se il rapporto è 99% - 1% provare prima a portarlo a 90% - 10%.\n",
        "\n",
        "<br>\n",
        "\n",
        "![oversampling](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F1*P93SeDGPGw0MhwvCcvVcXA.png&f=1&nofb=1)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://towardsdatascience.com/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb)\n",
        "\n",
        "<br>\n",
        "\n",
        "Per applicare queste trasformazioni in python esiste una libreria, [imblearn](https://imbalanced-learn.readthedocs.io/en/stable/api.html) che mette a disposizione molti tipi di bilanciamento. Nel nostro caso applicheremo un bilanciamento casuale delle osservazioni quindi non la useremo.\n",
        "\n",
        "Come stimatore verrà usata la regressione logistica.\n",
        "\n",
        "<br>\n",
        "\n",
        "![penguin](https://miro.medium.com/max/800/1*UgYbimgPXf6XXxMy2yqRLw.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/)\n",
        "\n",
        "<br>\n",
        "\n",
        "### Il Dataset\n",
        "\n",
        "Il dataset utilizzato è stato generato tramite Paysim, un simulatore di transazioni di carte di credito.\n",
        "\n",
        "Per motivi di spazio di github ne carico solo una minima parte, [l'intero dataset può essere scaricato da kaggle](https://www.kaggle.com/ntnu-testimon/paysim1).\n",
        "\n",
        "I campi del dataset sono:\n",
        "\n",
        "* step: simulazione dell'unità di tempo\n",
        "* type: il tipo di movimento. CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.\n",
        "* amount: ammontare della transazione\n",
        "* nameOrig: cliente che ha iniziato la transazione\n",
        "* oldbalanceOrg: saldo iniziale al momento della transazione\n",
        "* newbalanceOrig: saldo dopo la transazione\n",
        "* nameDest: destinatario della transazione\n",
        "* oldbalanceDest: saldo iniziale del ricevente\n",
        "* newbalanceDest: saldo del ricevente dopo la transazione\n",
        "* isFraud: variabile binaria che etichetta la frode (1) e il movimento regolare (0)\n",
        "* isFlaggedFraud: variabile binaria che controlla le anomalie, per anomalia in questo caso si intende un movimento di oltre 200K in una singola transazione.\n",
        "\n",
        "### Caricamento del dataset\n",
        "\n",
        "***Se stai usando il notebook su Colab esegui le prossime due celle, altrimenenti vai direttamente al caricamento con *read_csv* inserendo il path del tuo file *Paysim.csv***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMIOQelZh5Fz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9QTtyJfJrGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2aMzOIPgrpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/YOURPATH'\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Paysim.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbNMOsHLgrm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"Paysim.csv\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwxgSKUghFU0",
        "colab_type": "text"
      },
      "source": [
        "### Analisi del dataset\n",
        "\n",
        "Per prima cosa vediamo la percentuale di frodi nel dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnH2JpsxgrkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fraud_count = df.isFraud.value_counts()\n",
        "\n",
        "print(\"Not Fraud: \", fraud_count[0])\n",
        "print(\"Fraud: \", fraud_count[1])\n",
        "print(\"\\n\")\n",
        "print(\"Percentuale frodi: \", round(fraud_count[1]/fraud_count.sum())*100,2, \"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoQU_jknjWEr",
        "colab_type": "text"
      },
      "source": [
        "Il numero di frodi è bassissimo ma fortunatamente abbiamo abbastanza casi per applicare l'undersampling.\n",
        "\n",
        "Iniziamo col vedere che alcune variabili non sono informative per noi, possono essere tranquillamente eliminate:\n",
        "* step\n",
        "* nameOrig\n",
        "* nameDest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp7ONV1kjgTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(columns = ['step', 'nameOrig', 'nameDest'], inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E2E71_EmpbU",
        "colab_type": "text"
      },
      "source": [
        "### Analisi delle variabili\n",
        "\n",
        "Esercizio. Cercate di trovare relazioni nascoste tra i dati.\n",
        "\n",
        "* Influenza sulla variabile target.\n",
        "* Relazioni tra i regressori.\n",
        "* Analisi grafica\n",
        "\n",
        "Vedremo a lezione se con le soluzioni proposte il modello avrà performance migliori.\n",
        "\n",
        "### Preprocessing\n",
        "\n",
        "#### Missing Values\n",
        "\n",
        "Non sono presenti valori mancanti (il dataset è frutto di un generatore)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2Dx_a5zjgRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB43kkoaGppU",
        "colab_type": "text"
      },
      "source": [
        "#### Trasformazioni\n",
        "\n",
        "Abbiamo tre tipologie di variabili:\n",
        "* binaria\n",
        "* numerica continua\n",
        "* categorica\n",
        "\n",
        "Per la variabile binaria non c'è nessun tipo di traformazione da fare.\n",
        "\n",
        "Per le variabili numeriche applicheremo una normalizzazione per portarle tutte nel range [0,1].\n",
        "\n",
        "Studiamo la variabile categorica per capire che trasformazione applicare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irsokBBCjgPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.type.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeJ3t8wIGEX",
        "colab_type": "text"
      },
      "source": [
        "*type* ha 4 valori, quindi possiamo applicare un One-Hot Encoding.\n",
        "\n",
        "Per applicare le trasformazioni useremo uno strumento che rende il codice riutilizzabile ed elegante, il [ColumnTransormer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIm8LhpDImeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5n_UW06Imb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scelta degli attributi\n",
        "\n",
        "cat_attribs = [\"type\"]\n",
        "num_attribs = list(df._get_numeric_data().columns[0:-2]) # prendi tutte le colonne di tipo numerico e restituisce i nomi, tranne le ultime due"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhclfalDMhUJ",
        "colab_type": "text"
      },
      "source": [
        "Il ColumnTransformer non è altro che una lista di operazioni da compiere, ogni elemento della lista è composto da:\n",
        "* nome del passo\n",
        "* traformazione da applicare\n",
        "* colonne su cui applicarla\n",
        "\n",
        "La totale integrazione con scikit-learn fa si che questo elemento abbia gli stessi metodi che abbiamo visto fino ad ora:\n",
        "* fit\n",
        "* transform\n",
        "* fit_transform\n",
        "\n",
        "e basta applicarlo a tutto il dataset, questo riduce di molto la probabilità di errore dovute a replicazione del codice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nij1b0cUImY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = ColumnTransformer([\n",
        "                               (\"num\", MinMaxScaler(), num_attribs),\n",
        "                               (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "                               \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2rIeDSmNuIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_prepared = transform.fit_transform(df)\n",
        "df_prepared[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouTa1buENlXs",
        "colab_type": "text"
      },
      "source": [
        "Il trasformer ci restituisce una matrice che dovremo riconvertire in DataFrame per applicare le successive operazioni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zycpkmf-PDTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_complete = pd.concat([pd.DataFrame(df_prepared), df[[\"isFraud\", \"isFlaggedFraud\"]]], axis =1 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fffk328PDQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_complete.head()\n",
        "\n",
        "# esercizio: mettere il nome alle colonne"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhczuVOXoP1I",
        "colab_type": "text"
      },
      "source": [
        "### Modelling: dataset sbilanciato\n",
        "\n",
        "* baseline model\n",
        "* reglog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zJRWxwGPDPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_complete.drop(\"isFraud\", axis=1).copy()\n",
        "y = df_complete[\"isFraud\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfeB0FM9ReKa",
        "colab_type": "text"
      },
      "source": [
        "Visto che il dataset è fortemente sbilanciato applichiamo un campionamento stratificato sulla variabile target così da evitare che il train (o il test) set non presentino nessuna frode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOtOmLbCPDMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv7_3mRJRw3Q",
        "colab_type": "text"
      },
      "source": [
        "#### Baseline model\n",
        "\n",
        "Il baseline model è il nostro punto di partenza, in questo caso noi vogliamo un modello che classifica sempre la stessa etichetta. In questo caso l'accuratezza sarà altissima ma a noi interessa confrontarci su altre metriche:\n",
        "* precision\n",
        "* recall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drfKUr2sjgMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "dc = DummyClassifier(strategy = \"most_frequent\")\n",
        "\n",
        "dc.fit(train_x, train_y)\n",
        "\n",
        "yhat_dc = dc.predict(test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhfUJO0NUB_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "lr.fit(train_x, train_y)\n",
        "\n",
        "yhat_lr = lr.predict(test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGh6WPOEoZG4",
        "colab_type": "text"
      },
      "source": [
        "### Valutazioni"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UjpcOQVodG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPmfH-Afoc8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"BASELINE MODEL\", \"\\n\")\n",
        "print(confusion_matrix(test_y, yhat_dc))\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy: \", accuracy_score(test_y, yhat_dc))\n",
        "print(\"Precision: \", precision_score(test_y, yhat_dc))\n",
        "print(\"Recall: \", recall_score(test_y, yhat_dc))\n",
        "print(\"\\n\")\n",
        "print(classification_report(test_y, yhat_dc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiM_efgPX0YO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"LOGISTIC REGRESSION\", \"\\n\")\n",
        "print(confusion_matrix(test_y, yhat_lr))\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy: \", accuracy_score(test_y, yhat_lr))\n",
        "print(\"Precision: \", precision_score(test_y, yhat_lr))\n",
        "print(\"Recall: \", recall_score(test_y, yhat_lr))\n",
        "print(\"\\n\")\n",
        "print(classification_report(test_y, yhat_lr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2gccv3FYS_L",
        "colab_type": "text"
      },
      "source": [
        "### Bilanciare il dataset\n",
        "\n",
        "Effueremo un undersamplig completamente casuale, gli step sono:\n",
        "* dividere il dataset secondo la variabile target\n",
        "* creare un nuovo dataset con le osservazioni, estratte casualmente dal dataset più grande in modo che abbia lo stesso numero di ossevazioni della classe meno rappresentata\n",
        "* concatenare\n",
        "\n",
        "**N.B. Per l'esercizio abbiamo bilanciato perfettamente, nel mondo reale è più probabile che capiti di usare una percentuale diversa**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp8XO5k9YetB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "not_faud = df_complete[df_complete[\"isFraud\"]==0].copy()\n",
        "fraud = df_complete[df_complete[\"isFraud\"]==1].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egptYc6MY_hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "not_fraud_under = not_faud.sample(fraud.shape[0], replace=True, random_state=42)\n",
        "\n",
        "balanced_dataset = pd.concat([not_fraud_under, fraud], axis = 0)\n",
        "\n",
        "balanced_dataset.isFraud.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnn2xrDaa3Uk",
        "colab_type": "text"
      },
      "source": [
        "estraiamo train e test set, in questo caso non ci servirà stratificare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYByLq69a-XC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_bal = balanced_dataset.drop(\"isFraud\", axis=1).copy()\n",
        "y_bal = balanced_dataset[\"isFraud\"]\n",
        "\n",
        "balanced_train_x, balanced_test_x, balanced_train_y, balanced_test_y = train_test_split(X_bal, y_bal, test_size = 0.3, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PhrP7j8nYUhD"
      },
      "source": [
        "### Modelling: dataset bilanciato\n",
        "\n",
        "* baseline model\n",
        "* reglog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su1sNwwka-DC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cambia la strategia del classificatore baseline, non più il più frequente visto che le classi sono bilanciate\n",
        "\n",
        "dc_b = DummyClassifier()\n",
        "\n",
        "dc_b.fit(balanced_train_x, balanced_train_y)\n",
        "\n",
        "yhat_dc_b = dc_b.predict(balanced_test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEQuQKdEYSqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_b = LogisticRegression()\n",
        "\n",
        "lr_b.fit(balanced_train_x, balanced_train_y)\n",
        "\n",
        "yhat_lr_b = lr.predict(balanced_test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElPXYXhUcFBc",
        "colab_type": "text"
      },
      "source": [
        "### Valutazioni"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lryjoZPwcEVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"BASELINE MODEL\", \"\\n\")\n",
        "print(confusion_matrix(balanced_test_y, yhat_dc_b))\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy: \", accuracy_score(balanced_test_y, yhat_dc_b))\n",
        "print(\"Precision: \", precision_score(balanced_test_y, yhat_dc_b))\n",
        "print(\"Recall: \", recall_score(balanced_test_y, yhat_dc_b))\n",
        "print(\"\\n\")\n",
        "print(classification_report(balanced_test_y, yhat_dc_b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk0PKKBecrCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"LOGISTIC REGRESSION\", \"\\n\")\n",
        "print(confusion_matrix(balanced_test_y, yhat_lr_b))\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy: \", accuracy_score(balanced_test_y, yhat_lr_b))\n",
        "print(\"Precision: \", precision_score(balanced_test_y, yhat_lr_b))\n",
        "print(\"Recall: \", recall_score(balanced_test_y, yhat_lr_b))\n",
        "print(\"\\n\")\n",
        "print(classification_report(balanced_test_y, yhat_lr_b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkL5xPR6ePMc",
        "colab_type": "text"
      },
      "source": [
        "**A lezione commenti sul risultato ed eventuali proposte di miglioramento**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npNHJ-iTWEdA",
        "colab_type": "text"
      },
      "source": [
        "### Citazioni\n",
        "\n",
        "[PAYSIM: A FINANCIAL MOBILE MONEY SIMULATOR FOR FRAUD DETECTION](https://www.researchgate.net/publication/313138956_PAYSIM_A_FINANCIAL_MOBILE_MONEY_SIMULATOR_FOR_FRAUD_DETECTION)\n",
        "\n",
        "### Link Utili\n",
        "\n",
        "[Predicting Fraud in Financial Payment Services](https://www.kaggle.com/arjunjoshua/predicting-fraud-in-financial-payment-services)\n",
        "\n",
        "[Logistic Regression — Detailed Overview](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)\n",
        "\n",
        "[Fix imbalanced dataset](https://towardsdatascience.com/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb)"
      ]
    }
  ]
}