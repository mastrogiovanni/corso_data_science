{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_california_housing_price.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXD3FkPvmGtRWJ9nK/XSA7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickprock/corso_data_science/blob/devs/machine_learning_pills/01_supervised/01_california_housing_price.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYyg1dXPfoiX",
        "colab_type": "text"
      },
      "source": [
        "## California Housing Price\n",
        "\n",
        "<br>\n",
        "\n",
        "![housing](https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fstatic1.businessinsider.com%2Fimage%2F5b2a86cf1ae6624a008b5492-1190-625%2Fcalifornias-housing-market-has-reached-a-boiling-point-and-a-typical-home-costs-600000.jpg&f=1&nofb=1)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://www.businessinsider.com/california-home-price-hits-record-high-2018-6?IR=T)\n",
        "\n",
        "<br>\n",
        "\n",
        "Il primo esempio che vedremo è abbastanza noto in letteratura.\n",
        "\n",
        "**Date alcune caratteristiche delle case stimare il prezzo (la mediana in questo caso**.\n",
        "\n",
        "Il modello adottato è una regressione lineare:\n",
        "* i dati sono etichettati, ogni esempio ha il prezzo della casa, quindi siamo nel contesto dell' **approccio supervisionato**\n",
        "* la nostra **variabile target è numerica continua** quindi adottiamo la regressione\n",
        "\n",
        "Man mano verranno spiegate tutte le fasi del processo di definizione del progetto di Machine Learning (nei prossimi le varie fasi saranno molto meno approfondite).\n",
        "\n",
        "Questo notebook si basa sull'esempio al capitolo 2 di [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1SolLBcfmeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-22QGxlQjlgF",
        "colab_type": "text"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "Il dataset *California Housing Price* è stato usato nel 1997 per l'articolo *\"Sparse Spatial Autoregressions\" di Pace, R. Kelley and Ronald Barry, pubblicato in Statistics and Probability Letters.*\n",
        "\n",
        "Contiene 20640 osservazioni (poche per i modelli in produzione ma ottime per un esercizio) del 1990, si basa sul concetto di **block group** che è un'area geografica che và da circa 600 a 3000 persone utilizzato dal Census Bureau U.S., nel notebook il block groupè verrà chiamato *district*.\n",
        "\n",
        "Le variabili presenti:\n",
        "* latitudine\n",
        "* longitudine\n",
        "* housing median age\n",
        "* total rooms: il numero di camere nel distretto\n",
        "* total bedrooms: il numero totale di camere da letto nel distretto\n",
        "* population: la popolazione nel distretto\n",
        "* households: numero di famiglie nel distretto\n",
        "* median_income: reddito medio nel distretto\n",
        "* **median_house_value**\n",
        "* ocean_proximity: vicinanza all'oceano\n",
        "\n",
        "### Caricamento del dataset\n",
        "\n",
        "***Se stai usando il notebook su Colab esegui le prossime due celle, altrimenenti vai direttamente al caricamento con *read_csv* inserendo il path del tuo file *housing.csv***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ws7qeWdkFLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hiaTrKLkSUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/ENTER_YOUR_CODE'\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('housing.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0pJvmFFkYAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"housing.csv\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_McsTBTKoS_X",
        "colab_type": "text"
      },
      "source": [
        "### Analisi del dataset\n",
        "\n",
        "Ogni riga rappresenta un distretto e ha dieci attributi.\n",
        "\n",
        "La prima cosa che si nota è che sono tutte variabili numeriche tranne ocean_proximity, stampiamo le informazioni sul dataset per non incorrere in eventuali errori (es. valori numerici che in realtà sono stringhe)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNixr6sCpbG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixwm3Np1phWW",
        "colab_type": "text"
      },
      "source": [
        "Dalle info notiamo anche che la variabile total_bedrooms ha dei valori mancanti, sono una piccola percentuale in seguito vedremo come trattarli.\n",
        "\n",
        "Vediamo:\n",
        "* la distribuzione dei valori per ocean_proximity\n",
        "* le statistiche riassuntive per tutte le variabili numeriche"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOatWaguq_VQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.ocean_proximity.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-OOBN6uriyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(18,10))\n",
        "plt.bar(np.arange(len(df.ocean_proximity.value_counts())),df.ocean_proximity.value_counts())\n",
        "plt.xticks(np.arange(len(df.ocean_proximity.value_counts())), df.ocean_proximity.value_counts().index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMM-nkz6sC-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfE-nDbus4m0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.hist(bins=50, figsize=(20, 15))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvE7It1U4TWR",
        "colab_type": "text"
      },
      "source": [
        "Annotazione:\n",
        "* la variabile median_income non è espressa in dollari ma và da 0,5 a 15\n",
        "* sia l'età che il prezzo delle case raggiungono un massimo oltre il quale non vanno, quindi il modello non prevederà un prezzo oltre quel valore\n",
        "* gli attributi hanno scala molto differente tra loro\n",
        "* Guardando gli istogrammi si nota che molte variabili hanno una coda lunga, la distribuzione è tutta schiacciata a sinistra con una coda a destra molto pesante\n",
        "\n",
        "## Scikit-learn\n",
        "\n",
        "<br>\n",
        "\n",
        "![sklearn](https://scikit-learn.org/stable/_static/scikit-learn-logo-small.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://scikit-learn.org/stable/index.html)\n",
        "\n",
        "<br>\n",
        "\n",
        "Da qui in avanti useremo alcuni moduli e funzioni presenti nella libreria **scikit-learn**.\n",
        "\n",
        "Racchiude moduli per sviluppare un processo di machine learning, è semplice ed efficiente, basata (come pandas) su numpy e scipy, contiene modelli di:\n",
        "* regressione\n",
        "* classificazione\n",
        "* clustering\n",
        "\n",
        "La libreria è open source ([Licenza BSD](https://it.wikipedia.org/wiki/Licenze_BSD)) quindi utilizzabile ed estendibile.\n",
        "\n",
        "Per richiamarla in python si può usare il comando:\n",
        "```\n",
        "import sklearn\n",
        "```\n",
        "anche se solitamente non si fa vista la vastità della libreria, i  metodi complessi che racchiude e il derivante tempo di caricamento che ne comporterebbe. Solitamente si importa la funzione o il modulo di cui si hà bisogno.\n",
        "\n",
        "### Train - Test Split\n",
        "\n",
        "Per prima cosa dividiamo il dataset in train e test:\n",
        "* **train set**: la parte di dati su cui verrà *allenato* il modello. Su questi dati il modello calibrerà i suoi parametri basandosi sugli esempi a disposizione\n",
        "* **test set**:  la parte dei dati su cui verranno misurate le performance del modello.\n",
        "\n",
        "Solitamente questa suddivisione può essere 80-20 o 75-25 ma và effettuata dal data scientist a seconda del caso su cui si sta lavorando.\n",
        "\n",
        "I parametri che imposteremo sono:\n",
        "* il dataset\n",
        "* test_size: la percentuale di casi che vanno a finire nel test set\n",
        "* random_state: un numero che garantisce la replicabilità dell'esercizio, potete scegliere quale volete, cambiando numero cambia la distribuzione delle osservazioni.\n",
        "\n",
        "*train_test_split* applica un'estrazione casuale senza reimmissione.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bLqh1GS-vSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(df, test_size = 0.2, random_state = 42)\n",
        "\n",
        "print(\"train_set: \", \"\\n\", train_set)\n",
        "print(\"\\n\")\n",
        "print(\"test_set: \", \"\\n\", test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwftBBAGAKpA",
        "colab_type": "text"
      },
      "source": [
        "Abbiamo diviso le osservazioni in maniera assolutamente casuale, questa è la strada più semplice e anche quella esatta se ci sono abbastanza osservazioni da rendere la distribuzione dei casi abbastanza uniforme.\n",
        "\n",
        "Nel caso del nostro dataset non è così, la nostra variabile *median_income* (che è direttamente legata a quella di risposta) è numerica continua e ricade tra quelle con una coda molto lunga, possiamo vederne la distribuzione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYUjByQyA045",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.median_income.hist(bins=50, figsize=(18,10))\n",
        "plt.title(\"Median Income\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGm-QuMIIDjK",
        "colab_type": "text"
      },
      "source": [
        "Per ovviare a questo problema e avere tutti i livelli compresi sia nel train:\n",
        "* così il modello può allenarsi su quel caso\n",
        "\n",
        "sia nel test:\n",
        "* così possiamo valutare di quanto sbaglia in quel caso\n",
        "\n",
        "verrà usato il campionamento stratificato. Questa tecnica fa si che ogni campione sia rappresentativo dell'intera popolazione visto che ne riporta le medesime proporzioni.\n",
        "\n",
        "Essendo la nostra variabile numerica dovrà essere discretizzata e successivamente applicato il campionamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL9ce9zWIn7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"income_cat\"] = pd.cut(df[\"median_income\"], bins=[0.0,1.5,3.0,4.5,6.0,np.inf], labels=[1,2,3,4,5])\n",
        "df.income_cat.hist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t6HZXX8KHW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strat_train_set, strat_test_set = train_test_split(df, test_size = 0.2, random_state = 42, stratify = df.income_cat)\n",
        "\n",
        "# ora possiamo rimuovere la variabile che non ci serve più\n",
        "for set_ in (strat_train_set, strat_test_set):\n",
        "  set_.drop(columns=\"income_cat\", axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rbbZ8zyXWQI",
        "colab_type": "text"
      },
      "source": [
        "### Analisi della correlazione\n",
        "\n",
        "Vediamo come sono distribuite geograficamente le nostre abitazioni e successivamente facciamo un'analisi della correlazione delle variabili indipendenti con quella target.\n",
        "\n",
        "La gradazione del colore, dal blu al rosso è data dal valore, la grandezza del punto dalla densità di popolazione nel blocco.\n",
        "\n",
        "Questa analisi verrà fatta solo sul train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T4DV0mcXovy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "house = strat_train_set.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m77yEGI2Xtuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "house.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=house[\"population\"]/100, label=\"Population\", figsize=(18,10),\n",
        "           c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qERzW7vNHuhl",
        "colab_type": "text"
      },
      "source": [
        "Come si può vedere le case più costose stanno sulla costa come era facilmente prevedibile.\n",
        "\n",
        "Ora conduciamo un'**analisi di correlazione**.\n",
        "\n",
        "La correlazione che utilizzeremo è quella di Pearson, si applica a dati numerici continui e rileval la relazione **lineare** che c'è tra le variabili. \n",
        "L'indice di correlazione varia tra -1 e 1:\n",
        "* -1 massima correlazioni negativa\n",
        "* 1 massima correlazione positiva\n",
        "* 0 nessuna correlazione\n",
        "\n",
        "Ci può dire:\n",
        "* quali sono le variabili che impattano maggiormente sul target\n",
        "* se ci sono variabili fortemente correlate tra loro e quindi informazioni ridondanti."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixqbg44xI-sQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr_matrix = house.corr()\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWeoyS6mKFrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot delle più correlate\n",
        "pd.plotting.scatter_matrix(house[[\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]], figsize=(18,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqb_19B4LBO8",
        "colab_type": "text"
      },
      "source": [
        "La variabile più importante per prevedere il valore mediano della casa è lo stipendio mediano del blocco in cui sorge.\n",
        "\n",
        "## Cleaning\n",
        "\n",
        "Inizia la fase di cleaning, per prima cosa ci creiamo una copia del train su cui effettuare la pulizia.\n",
        "\n",
        "Dividiamo le variabili indipendenti dal target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T7BTwQ4LzR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "house = strat_train_set.drop(\"median_house_value\", axis=1).copy()\n",
        "house_label = strat_train_set[\"median_house_value\"].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WPawgAXMjAJ",
        "colab_type": "text"
      },
      "source": [
        "### Imputare i valori mancanti\n",
        "\n",
        "La prima procedura da fare è decidere come lavorare i valori mancanti, questo è un passaggio fondamentale perchè **qualsiasi strategia scegliamo introduce errore sistematico** nel dataset, l'unica cosa che possiamo fare è scegliere la strategia che lo minimizza.\n",
        "\n",
        "Strategie che è possibile adottare:\n",
        "* se la percetuale di valori mancanti è alta eliminare la variabile\n",
        "* eliminare l'intera osservazione (alta perdita di informazione)\n",
        "* inserire 0 al posto del valore mancante (alto errore sistematico introdotto)\n",
        "* sostituire il valore con media, mediana o valore più frequente (strategia consigliata)\n",
        "\n",
        "Scikit-learn ha funzioni apposite, si chiamano **Imputer** in questo caso non le useremo, ma sostituremo ugualmente i valori mancanti con la mediana della variabile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqGuuJamPN88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calcoliamo la mediana sul train\n",
        "median = house[\"total_bedrooms\"].median()\n",
        "house[\"total_bedrooms\"].fillna(median, inplace = True)\n",
        "# la variabile mediana ci servirà in seguito per sostiture i valori nel test set, infatti non vengono ricalcolati ma solo imputati"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFDlIdRpQOUG",
        "colab_type": "text"
      },
      "source": [
        "### Convertire le stringhe\n",
        "\n",
        "La seconda operazione è quella di convertire le stringhe in valori numerici. Ci sono molti metodi di conversione, noi in questo caso useremo il **One-Hot Encoding**.\n",
        "\n",
        "<br>\n",
        "\n",
        "![onehot](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fxamlbrewer.files.wordpress.com%2F2019%2F04%2Fone-hot_encoding.png&f=1&nofb=1)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://xamlbrewer.wordpress.com/2019/04/23/machine-learning-with-ml-net-in-uwp-field-aware-factorization-machine/)\n",
        "\n",
        "<br>\n",
        "\n",
        "Per ogni osservazione si crea un vettore di lunghezza N pari al numero di categorie nella nostra variabile, ogni elemento avrà valore 0 se l'osservazione non appartiene alla categoria, 1 se appartiene.\n",
        "\n",
        "***N.B. a differenza di quanto succede nella statistica classica in cui per una variabile categoriale con M attributi bastano M-1 dummies nel machine learning è buona norma tenerle tutte (questa caratteristica si accentua nelle reti neurali).***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlwGdO1CRQhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "house_cat = house[[\"ocean_proximity\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOMKO-7URvXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzswD3JXR7Zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "house_cat_onehot = encoder.fit_transform(house_cat)\n",
        "\n",
        "print(house_cat_onehot.toarray())\n",
        "print(\"\\n\")\n",
        "print(encoder.categories_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEA3aZaDUSvk",
        "colab_type": "text"
      },
      "source": [
        "### Scaling\n",
        "\n",
        "Gli algoritmi di machine learning non digeriscono bene le variabili numeriche con scala differente tra loro. Per ovviare a questo i metodi più utilizzati sono:\n",
        "* **min-max** scaling (anche detta normalizzazione) i valori vengono portati in una scala [0,1], scikit-learn permette anche di personalizzare questo intervallo.\n",
        "* **standard** scaling, ovvero ogni osservazione meno la media e diviso al deviazione standard.\n",
        "\n",
        "Useremo il secondo approccio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fkGWcpDR7FV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN337fStVq0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "house.drop(\"ocean_proximity\", axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5j3GG4kV8C3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "house_stand = pd.DataFrame(scaler.fit_transform(house))\n",
        "house_stand.columns = house.columns\n",
        "house_stand.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzvJEcN_WJBs",
        "colab_type": "text"
      },
      "source": [
        "Ora dobbiamo concatenare i valori numerici preprocessati con la variabile categorica dopo il onehot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fAqxuGidO-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "house_processed = pd.concat([house_stand, pd.SparseDataFrame(house_cat_onehot.toarray(), columns=['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], default_fill_value=0)], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_BeuzHReXoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "house_processed.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oVcrb4hj_mV",
        "colab_type": "text"
      },
      "source": [
        "## Training and Evaluation\n",
        "\n",
        "Per risolvere questo problema ho scelto il modello di regressione più semplice, la **regressione lineare**, per approfondire altri modelli consultare i link utili.\n",
        "\n",
        "<br>\n",
        "\n",
        "![lr](https://files.realpython.com/media/fig-lin-reg.a506035b654a.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://realpython.com/linear-regression-in-python/)\n",
        "\n",
        "<br>\n",
        "\n",
        "Non faremo tuning degli iperparametri del modello, ma prenderemo quelli di default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvYjnZjxkaCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3elFnCyBkoVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr.fit(house_processed, house_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZtWi3v5k9p3",
        "colab_type": "text"
      },
      "source": [
        "Ora abbiamo allenato il modello sul train, dobbiamo preparare il test set per misurare le performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pct45Reok7Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_x = test_set.drop(\"median_house_value\", axis= 1).copy()\n",
        "test_y = test_set[\"median_house_value\"].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wizH8bOalnxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# missing values\n",
        "test_x[\"total_bedrooms\"].fillna(median, inplace = True)\n",
        "\n",
        "# one-hot encoding\n",
        "test_cat_onehot = pd.SparseDataFrame(encoder.transform(test_x[[\"ocean_proximity\"]]).toarray(), columns=['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], default_fill_value=0)\n",
        "\n",
        "# scaling\n",
        "test_x.drop(\"ocean_proximity\", axis=1, inplace=True)\n",
        "\n",
        "test_x_stand = pd.DataFrame(scaler.transform(test_x))\n",
        "test_x_stand.columns = test_x.columns\n",
        "\n",
        "# unione\n",
        "test_x_processed = pd.concat([test_x_stand, test_cat_onehot], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJeQM0QanRRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yhat = lr.predict(test_x_processed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj2BK4I4nXBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yhat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzIH1YMYnZV_",
        "colab_type": "text"
      },
      "source": [
        "#### Valutazione del modello\n",
        "\n",
        "Per valutare un modello di regressione abbiamo due metriche fondamentali:\n",
        "* errore\n",
        "* R^2\n",
        "\n",
        "Per quanto riguarda l'errore può essere utilizzato o la radice dell'errore quadratico medio o l'errore assoluto medio, dipende dai casi, solitamente si utilizza il primo perchè **rafforza** le distanze tra previsione e valore reale, ma in presenza di molti outliers nei dati e da preferire quello assoluto.\n",
        "\n",
        "Nel nostro caso useremo il RMSE.\n",
        "\n",
        "<br>\n",
        "\n",
        "![RMSE](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F0*CRZU7qETwW3bwBwK.gif&f=1&nofb=1)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credit](https://towardsdatascience.com/understanding-objective-functions-in-neural-networks-d217cb068138)\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cydundjkpROZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(test_y, yhat))\n",
        "print(rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhq2qHn3syOv",
        "colab_type": "text"
      },
      "source": [
        "Questo non è un grande risultato, vuol dire che il nostro modello sbaglia in media di quasi 70K$, siamo in un caso di **underfitting**.\n",
        "\n",
        "Si dice che un modello và in **underfitting**:\n",
        "* quando i dati non sono abbastanza esplicativi per il fenomeno\n",
        "* quando le ipotesi alla base del modello sono troppo semplici e quest'ultimo non riesce a catturare i pattern nei dati.\n",
        "\n",
        "Al contrario un modello và in **overfitting** quando non riesce a generalizzare e si adatta troppo bene al training set.\n",
        "\n",
        "<br>\n",
        "\n",
        "![fit](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F1200%2F1*_7OPgojau8hkiPUiHoGK_w.png&f=1&nofb=1)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76)\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Cosa si può fare per migliorare la previsione?\n",
        "\n",
        "Non potendo aumentare il campione di dati possiamo solo usare modelli più potenti e cercare di fare un tuning più preciso.\n",
        "\n",
        "Per il corso, alla fine delle lezioni potete:\n",
        "\n",
        "1) Provare questi due modelli:\n",
        "  * Random Forest Regressor\n",
        "  * XGBoost Regressor\n",
        "\n",
        "2) Scegliere i valori degli iperparametri tramite cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyshhOKIiJw7",
        "colab_type": "text"
      },
      "source": [
        "### Link utili\n",
        "\n",
        "[Repo Github del libro di Aurélien Gèron](https://github.com/ageron/handson-ml2)\n",
        "\n",
        "[California Housing Price su Kaggle](https://www.kaggle.com/harrywang/housing#housing.csv)\n",
        "\n",
        "[Underfitting and Overfitting](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76)\n",
        "\n",
        "[Imputation of missing values](https://scikit-learn.org/stable/modules/impute.html#impute)"
      ]
    }
  ]
}