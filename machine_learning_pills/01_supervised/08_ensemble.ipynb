{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOeC47zjm0z9xhm/mSkl7jB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickprock/corso_data_science/blob/devs/machine_learning_pills/01_supervised/08_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZfT-5JjYTM0",
        "colab_type": "text"
      },
      "source": [
        "# Algoritmi Ensemble\n",
        "\n",
        "<br>\n",
        "\n",
        "![mush](https://s3-eu-central-1.amazonaws.com/hermanns-website-2/wp-content/uploads/2018/11/09142320/florian-van-duyn-383221-unsplash_resized.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://www.hermanns.com/why-mushrooms-talk-to-us/)\n",
        "\n",
        "<br>\n",
        "\n",
        "Fino ad ora abbbiamo visto quelli che vengono detti *modelli deboli* o *weak learner*.\n",
        "\n",
        "Cosa vuol dire in sostanza. Se noi costruiamo un albero di classificazione quello è solo uno dei diversi modi che esistono per classificare quelle osservazioni. I classificatori deboli sono anche inclini ad andare in overfitting proprio perchè la loro struttura è una delle molteplici che si potrebbero generare.\n",
        "\n",
        "Una soluzione a questo problema sono i modelli ***Ensemble*** che mettono insieme più modelli deboli per generare un modello robusto al rumore. Naturalmente i costi in terminin di risorse e tempi salgono.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "Il dataset che useremo si chiama Mushrooms e classifica i funghi in velenosi e non velenosi. Potete trovare maggiorni informazioni sulla[ pagina dedicata dell'UCI ML.](http://archive.ics.uci.edu/ml/datasets/Mushroom)\n",
        "\n",
        "Il dataset ha oltre 8000 osservazioni e 23 variabili tutte categoriche. Faremo un minimo di conversione sulle variabili ma nessuna analisi dei dati o preprocessing.\n",
        "\n",
        "Il dataset e il codice sono presi dal [kaggle kernel a questo link](https://www.kaggle.com/drvader/random-forests-and-gradient-boosting/data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5KTW0JRgn_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tByPSh5gyt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqbI6h1Ag3Ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1bo0cQb7hXjx1HRK6wj1G9kkt5ErVP9bN'\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('mushrooms.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXTS-JWng5ab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"mushrooms.csv\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUR-Czhhc7HD",
        "colab_type": "text"
      },
      "source": [
        "# Preparazione del dataset\n",
        "\n",
        "Costruiamo la variabile di risposta binaria:\n",
        "* 0 non velenoso\n",
        "* 1 velenoso\n",
        "\n",
        "Applichiamo un OneHot Encoding su tutte le altre variabili."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HReJx8TgvWxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"class\"].replace([\"e\", \"p\"], [1, 0], inplace= True)\n",
        "y = df[\"class\"]\n",
        "df.drop(\"class\", axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtS3P2SobOGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = pd.get_dummies(df)\n",
        "X.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQklJL6ydv6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state= 42)\n",
        "print(\"Osservazioni train: \", len(X_train), \"\\n\", \"Osservazioni Test: \", len(X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhWByb6AeuLa",
        "colab_type": "text"
      },
      "source": [
        "## Models\n",
        "\n",
        "**N.B. Per questioni di tempi non faremo tuning dei parametri ma potete trovare riferimenti interessanti tra i link in fondo al notebook.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQrDBt81gbiJ",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest\n",
        "\n",
        "<br>\n",
        "\n",
        "![RandomForest](https://miro.medium.com/max/2000/1*_B5HX2whbTs3DS8M6YBD_w@2x.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)\n",
        "\n",
        "<br>\n",
        "\n",
        "Il **Random Forest** è l'algoritmo che ha sicuramente il nome più affascinante di tutti.\n",
        "\n",
        "Usa una tecnica chiamata *bagging* per allenare in maniera parallela N decision tree e successivamente con un sistema di pesi (banalmente il più frequente) sceglie la classificazione finale. Dall'allenamento di N decision tree deriva il nome di questo algoritmo.\n",
        "\n",
        "#### Bagging\n",
        "\n",
        "Il **Bagging** (o Bootstrap aggregation) è una tecnica che si basa sul bootstrap, ovvero un ricampionamento in cui da una serie di dati vengono estratti N campioni indipendenti e ogni campione viene dato come input ad uno degli N alberi.\n",
        "\n",
        "<br>\n",
        "\n",
        "![bootstrap](https://miro.medium.com/max/1400/1*lWnm3eJVe3uo95OcSg5jUA@2x.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)\n",
        "\n",
        "<br>\n",
        "\n",
        "In questo modo ogni albero protrebbe essere differente e il risultato molto più resistente all'overfitting.\n",
        "\n",
        "Inoltre allenando più alberi in maniera indipendente vi è una selezione delle features più impattanti."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ed63uMnfd84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rfcl = RandomForestClassifier(n_estimators = 20, random_state = 42)\n",
        "\n",
        "rfcl.fit(X_train, y_train)\n",
        "\n",
        "yhat_rfcl = rfcl.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY9GhxX9q3Je",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Boosting\n",
        "\n",
        "<br>\n",
        "\n",
        "![boosting](https://miro.medium.com/max/2000/1*VGSoqefx3Rz5Pws6qpLwOQ@2x.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Boosting\n",
        "\n",
        "A differenza del bagging il **Boosting** è un processo iterativo che migliora la stima ad ogni passo fino ad arrivare ad un modello finale. Ad esempio vediamo questi due passi:\n",
        "\n",
        "**STEP 1**:\n",
        "\n",
        "* abbiamo una serie di dati\n",
        "* proviamo a fare fitting con una funziona logaritmica\n",
        "\n",
        "<br>\n",
        "\n",
        "![step1](https://iaml.it/blog/gradient-boosting/images/P4SrKe6.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://iaml.it/blog/gradient-boosting)\n",
        "\n",
        "<br>\n",
        "\n",
        "**STEP 2**:\n",
        "\n",
        "* per un miglior fitting aggiungiamo una funzione seno che cattura gli andamenti oscillatori\n",
        "\n",
        "<br>\n",
        "\n",
        "![step2](https://iaml.it/blog/gradient-boosting/images/7EUfGSi.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://iaml.it/blog/gradient-boosting)\n",
        "\n",
        "<br>\n",
        "\n",
        "Questo è un esempio di come funzionano gli algortimi di boosting, un processo iterativo dove ad ogni passo si cerca di migliorare il fitting.\n",
        "\n",
        "Esistono diversi algoritmi di boosting, i più utilizzati e noti sono:\n",
        "* AdaBoost\n",
        "* GradientBoost (da cui deriva [XGBoost](https://en.wikipedia.org/wiki/XGBoost) che è molto prababilmente l'algoritmo più vincente su Kaggle)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x2XZDBXq1EK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gbcl = GradientBoostingClassifier(n_estimators = 20, random_state = 42)\n",
        "\n",
        "gbcl.fit(X_train, y_train)\n",
        "\n",
        "yhat_gbcl = rfcl.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhrZVYC2E8n3",
        "colab_type": "text"
      },
      "source": [
        "### Valutazione delle performance\n",
        "\n",
        "#### Curva di ROC\n",
        "\n",
        "La curva di ROC è uno strumento per valutare i classificatori binari e consite, in termini semplici, nel rapporto tra allarmi veri e falsi allarmi.\n",
        "\n",
        "Misura la porzione dei veri positivi trovati rispetto a quella dei falsi postitivi, un valore pari a 1 corrisponde ad una perfetta classificazione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHrcFXCJE6VT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, plot_roc_curve"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5GO4ofRF68C",
        "colab_type": "text"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWZhDeadF5x-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(confusion_matrix(y_test, yhat_rfcl))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(accuracy_score(y_test, yhat_rfcl))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fExR5O9Hd9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_roc_curve(rfcl, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIRG9sgAM-I_",
        "colab_type": "text"
      },
      "source": [
        "#### Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UTqiyJHM91E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(confusion_matrix(y_test, yhat_gbcl))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(accuracy_score(y_test, yhat_gbcl))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdQG6lNKHmQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_roc_curve(gbcl, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4gWTkllIU3I",
        "colab_type": "text"
      },
      "source": [
        "## Link Utili\n",
        "\n",
        "[Kaggle kernel](https://www.kaggle.com/drvader/random-forests-and-gradient-boosting/data)\n",
        "\n",
        "[Gradient Boosting - IAML](https://iaml.it/blog/gradient-boosting)\n",
        "\n",
        "[Ensemble methods: bagging, boosting and stacking](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)\n",
        "\n",
        "[Gradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost](https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/)\n",
        "\n",
        "[Present The Feature Importance of A Random Forest Classifier](https://towardsdatascience.com/present-the-feature-importance-of-the-random-forest-classifier-99bb042be4cc)\n",
        "\n",
        "[Hyperparameters Optimization for LightGBM, CatBoost and XGBoost Regressors using Bayesian Optimization.](https://medium.com/analytics-vidhya/hyperparameters-optimization-for-lightgbm-catboost-and-xgboost-regressors-using-bayesian-6e7c495947a9)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nzh2anoJII7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}