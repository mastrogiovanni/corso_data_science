{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "02_classificare_lo_spam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickprock/corso_data_science/blob/devs/machine_learning_pills/01_supervised/02_classificare_lo_spam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIL-D2ZtKVzk",
        "colab_type": "text"
      },
      "source": [
        "## Classificare lo spam\n",
        "\n",
        "<br>\n",
        "\n",
        "![spam](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fstorage.googleapis.com%2Fcdn-leanplum-images%2F1%2F2019%2F01%2FEmail-marketing-spam-feature-min.jpg&f=1&nofb=1)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://www.leanplum.com/blog/email-marketing-campaign-spam/)\n",
        "\n",
        "<br>\n",
        "\n",
        "La classificazione delle e-mail di spam è stato uno dei primi problemi \"di massa\" su cui si è cimentato il machine learning.\n",
        "\n",
        "Per individuare i messaggi di spam vengono utilizzati i classificatori binari, una tipo di modelli che discriminano tra due classi, nel nostro caso:\n",
        "* SPAM\n",
        "* NON SPAM\n",
        "\n",
        "Questi fanno parte dei modelli **supervisionati** avendo bisogno di esempi etichettati per addestrare i modelli ed è un problema di **classificazione** visto che la variabile target è binaria.\n",
        "\n",
        "Questo notebook è derivato da un notebook di [Valentina Porcu](https://github.com/valentinap), tra i link utili vi lascerò il suo corso di Data Science in Python su Udemy.\n",
        "\n",
        "### NLTK\n",
        "[Natural Language ToolKit](https://www.nltk.org) è una libreria vastissima per il NLP, la useremo in fase di pulizia del testo perchè contiene molti metodi lavorare sui testi ed è facile da installare e utilizzare e supporta moltissime lingue avendo oltre 50 lessici."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnKeEBK-KVzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejeNkegjQN9G",
        "colab_type": "text"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "Il dataset utilizzato è [SMS SPAM Collection](http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) disponibile dal repositori dell'[UCI Machine Learning](https://archive.ics.uci.edu/ml/index.php).\n",
        "\n",
        "Ha solo due colonne:\n",
        "* **label**: {spam, ham}\n",
        "* **text**: il testo del messaggio\n",
        "\n",
        "### Caricamento del dataset\n",
        "\n",
        "***Se stai usando il notebook su Colab esegui le prossime due celle, altrimenenti vai direttamente al caricamento con *read_csv* inserendo il path del tuo file *SMSSpamCollection.csv***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh4xm25RRQck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E6VoK2jRix2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/YOUR_CODE'\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('SMSSpamCollection')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZsVrK5pKV0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('SMSSpamCollection', sep = \"\\t\", names=[\"label\", \"text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp75cHmPKV0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33KTEGnETQ2a",
        "colab_type": "text"
      },
      "source": [
        "Utilizzando la funzione describe per i gruppi possiamo vedere che:\n",
        "* ci sono 5572 messaggi, di cui 5169 unici\n",
        "* lo spam è il 13.4% del dataset\n",
        "* il messaggio di spam più comune invita a contattare il servizio clienti."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXUs0abOKV0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df.groupby('label').describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdGrBbDuT9aI",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "<br>\n",
        "\n",
        "![nlp_prep](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.kdnuggets.com%2Fwp-content%2Fuploads%2Ftext-preprocessing-framework-2.png&f=1&nofb=1)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html)\n",
        "\n",
        "<br>\n",
        "\n",
        "Il preprocessing dei testi ha delle sue particolari sequenze di operazioni per renderlo \"appetibile\" agli algoritmi. Per eseguirle useremo nltk.\n",
        "\n",
        "La pulizia si basa su:\n",
        "* sostituire le maiuscole con le minuscole così da uniformare i termini\n",
        "* eliminare numeri, punteggiatura, caratteri speciali\n",
        "* eliminare le stopwords, ovvero quelle parole che appaiono molto spesso nel testo ma non danno particolari informazioni (es. articoli, congiunzioni, ...)\n",
        "* applicare lo stemming, ovvero ridurre le parole alla loro radice così da no avere termini differenti per singolare/plurale, maschile/femminile, coniugazione di verbi.\n",
        "\n",
        "Prima di condurre qualsiasi operazione sui testi però trasformiamo la label da stringa a valore numerico:\n",
        "* ham: 0\n",
        "* spam: 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru7ZXj_bKV1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cl = {'ham': 0, 'spam': 1}\n",
        "df['label'] = df['label'].map(cl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLTON4ohKV1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CWBh1LNKV08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')\n",
        "sw = set(nltk.corpus.stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6P3h71MKV1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxPyzp5uKV1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "corpus = []\n",
        "for i in range(0, df.shape[0]):\n",
        "    text = re.sub('[^a-zA-Z]', ' ', df['text'][i]) # mantieni solo il testo\n",
        "    text = text.lower() # tutto minuscolo\n",
        "    text = text.split() # ogni frase un vettore di parole\n",
        "    ps = PorterStemmer() # stemmer\n",
        "    text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))] # elimina le stopword e applica lo stemming\n",
        "    text = ' '.join(text) # ricostruisce la frase\n",
        "    corpus.append(text) # ricostruisce il dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u0vdCwicnKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7rJXBgDc6Tu",
        "colab_type": "text"
      },
      "source": [
        "### Bag of Words\n",
        "\n",
        "Per passare il testo agli algoritmi dobbiamo trasformarlo in una matrice, in questo caso utilizziamo la tacnica del **bag-of-words**.\n",
        "Viene creato un dizionario di termini, pero gni frase si annota la presenza/assenza del termine e la frequenza, ogni frase sarà la riga di una *Document - Term Matrix*.\n",
        "\n",
        "<br>\n",
        "\n",
        "![bow](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F0*jdeMyO7-xpbvvi4b.jpg&f=1&nofb=1)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://medium.com/greyatom/an-introduction-to-bag-of-words-in-nlp-ac967d43b428)\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdEtkmZwKV1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeVJb2TIKV1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv = CountVectorizer(max_features = 2000) # per comodità il nostro vocabolario conterrà solo i 2000 termini più frequenti\n",
        "x = cv.fit_transform(corpus).toarray()\n",
        "cl = df['label'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKlPVc3HKV11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfuhsEJvKV17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, cl, test_size = 0.3, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjenKeBketqx",
        "colab_type": "text"
      },
      "source": [
        "### Regressione Logistica\n",
        "\n",
        "La regressione logistica, è un modello di regressione non lineare utilizzato quando la variabile dipendente è di tipo dicotomico.\n",
        "A dispetto del nome è un modello di classificazione binaria infatti il suo output è una probabilità di appartenenza/non appartenenza ad una categoria.\n",
        "\n",
        "Viene considerato come l'entry level del Machine Learning, ed è utilizzato soprattutto per:\n",
        "* fraud detection\n",
        "* predictive maintenance\n",
        "* classificazione di documenti\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "![penguin](https://miro.medium.com/max/800/1*UgYbimgPXf6XXxMy2yqRLw.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/)\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pyMrzhGKV2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bzKClPwKV2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZQFfCp9KV2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGZXKLDxKV2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_pred = lr.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7BrTQHkfPXb",
        "colab_type": "text"
      },
      "source": [
        "### Valutazione del modello\n",
        "\n",
        "Per la valutazione del modello di classificazione abbiamo diversi strumenti e diverse misure.\n",
        "\n",
        "Il primo strumento è la **confusion matrix**.\n",
        "\n",
        "<br>\n",
        "\n",
        "![cm](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.stack.imgur.com%2FysM0Z.png&f=1&nofb=1)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://tex.stackexchange.com/questions/20267/how-to-construct-a-confusion-matrix-in-latex)\n",
        "\n",
        "<br>\n",
        "\n",
        "La confusion matrix mette in relazione i valori osservati con quelli stimati dal modello ed è lo strumento più immediato di valutazione in quanto ci permette di calcolare le seguenti metrriche:\n",
        "* accuratezza (accuracy): (TP + TN)/TotalCase ovvero i casi in cui il modello ha predetto bene, fratto i casi totali. Questa è la misura più utilizzata ma attenzione perchè per dataset sbilanciati, anche più del nostro, pur non facendo stime corrette si può arrivare ad una accuratezza molto alta.\n",
        "* precisione (precision): TP/(TP + FP) ovvero quanto è preciso accurato il nostro modello quando stima una classe, quanti elementi tra gli stimati ne fanno effettivamente parte? Ad esempio se etichetta come spam un'email non spam, l'utente potrebbe perdere informazioni.\n",
        "* richiamo (recall): TP/(TP + FN), misura il *costo del modello*, ad esempio misura quante attività fraudolente vengono classificate come non-fraudolente.\n",
        "* F1: 2 x [(Precision x Recall)/(Precision + recall)], è una misura che valuta l'equilibrio tra le precedenti, è usata spesso per valutare modelli diversi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP2lTcRxKV2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPOpkMM1KV2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(confusion_matrix(y_test, lr_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txg9OYzWKV2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(y_test, lr_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X5rkk0S56FP",
        "colab_type": "text"
      },
      "source": [
        "In questo caso l'accuratezza è buona, 98% perchè solo 29 casi su 1672 vengono classificati male.\n",
        "\n",
        "La precisione non è male, abbiamo solo 3 e-mail non spam che vengono etichettate come spam.\n",
        "\n",
        "Cosa diversa per la recall, in questo caso il filtro anti-spam, non funziona bene, ci sono sfuggite 26 e-mail!\n",
        "\n",
        "La F1-score totale del modello è 96% ma risente dello sbilanciamento del dataset, quindi valutando solo quella sull'etichetta spam abbiamo il 93%.\n",
        "\n",
        "### Support Vector Machines\n",
        "\n",
        "Le Support Vector Machine sono un modello di classificazione/regressione che basano il loro funzionamento su delle funzioni *kernel* che suddiviono lo spazio in cui sono presenti le osservazioni. Funzionano molto bene per classificazioni binarie, sono state utilizzate anche per altri tipi di classificazioni e per la regressione ma non con gli stessi risultati.\n",
        "\n",
        "Nel nostro caso useremo una funzione kernel lineare.\n",
        "\n",
        "<br>\n",
        "\n",
        "![svm](https://upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Svm_max_sep_hyperplane_with_margin.png/360px-Svm_max_sep_hyperplane_with_margin.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfSFZRJzKV24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRlrjNkOKV27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBSOhWyYKV3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = SGDClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF-mrbJzKV3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVTKr3QsKV3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_pred = clf.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX0lziH-_5Sg",
        "colab_type": "text"
      },
      "source": [
        "### Valutazione del modello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqYuamW-KV3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(confusion_matrix(y_test, clf_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKwx1UCEKV3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(y_test, clf_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyB89ivJ_8yZ",
        "colab_type": "text"
      },
      "source": [
        "Lascio a voi i commenti\n",
        "\n",
        "### Naive Bayes\n",
        "\n",
        "<br>\n",
        "\n",
        "![Bayes](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fthatware.co%2Fwp-content%2Fuploads%2F2019%2F03%2Fnaive-bayes.png&f=1&nofb=1)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://thatware.co/naive-bayes/)\n",
        "\n",
        "<br>\n",
        "\n",
        "Il naive bayes è un classificatore molto semplice che si basa sull'omonimo teorema.\n",
        "\n",
        "Per le ipotesi abbastanza semplici alla sua base è molto utile per piccoli set di dati e viene usato spesso nella classificazione dei documenti.\n",
        "\n",
        "Con gli iperparamentri opportunamente regolati può competere con modelli più complessi come le SVM.\n",
        "\n",
        "In scikit-learn sono presenti diversi classificatori bayesiani a seconda della distribuzione del dato, qui ne testeremo 2, vi anticipo già che il *MultinomialNB* è quello più utilizzato per la classificazione dei documenti.\n",
        "\n",
        "\n",
        "##### GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17bJ5PhcKV3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5E6rL0UKV3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb = GaussianNB()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiLqrwJzKV3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmXh632pKV3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_pred = nb.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aal3Cc7BDqxL",
        "colab_type": "text"
      },
      "source": [
        "##### MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL9f7pmiKV39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1twj01bDKV4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb2 = MultinomialNB()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toCbCXsqKV4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb2.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxNvZXXfKV4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb2_pred = nb2.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwjIfDHaBgIq",
        "colab_type": "text"
      },
      "source": [
        "#### Valutazione dei modelli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9bWXmb_KV3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(confusion_matrix(y_test, nb_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn-4tdynKV32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(y_test, nb_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHUJyvnuKV4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(confusion_matrix(y_test, nb2_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrubyNP3KV4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(y_test, nb2_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltrf96H9B9M7",
        "colab_type": "text"
      },
      "source": [
        "Lascio a voi la valutazione del modello\n",
        "\n",
        "### Conclusioni\n",
        "\n",
        "Ora è il momento di scegliere il classificatore migliore per questo problema, qual è?\n",
        "\n",
        "Come potrebbe essere migliorata la prestazione?\n",
        "\n",
        "Come potremmo valutare se siamo in una situazione di overfitting?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAcbQej3U7rR",
        "colab_type": "text"
      },
      "source": [
        "## Link Utili\n",
        "\n",
        "[A general approach to preprocessing text data](https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html)\n",
        "\n",
        "[Corso completo di data science con Python](https://www.udemy.com/course/data-science-con-python/)\n",
        "\n",
        "[Introduction to bag of words](https://medium.com/greyatom/an-introduction-to-bag-of-words-in-nlp-ac967d43b428)\n",
        "\n",
        "[Gentle introduction to bag of words](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)\n",
        "\n",
        "[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "\n",
        "[Accuracy, Precision, Recall, F1](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)\n",
        "\n",
        "[Naive Bayes](https://machinelearningmastery.com/naive-bayes-for-machine-learning/)"
      ]
    }
  ]
}