{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPauK22mWMagduKzBQQllf0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickprock/corso_data_science/blob/master/machine_learning_pills/02_unsupervised/04_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ashzg6OklbF2"
      },
      "source": [
        "# Clustering\n",
        "\n",
        "<br>\n",
        "\n",
        "![clustering](https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_0011.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://scikit-learn.org/stable/modules/clustering.html)\n",
        "\n",
        "<br>\n",
        "\n",
        "## Definizione\n",
        "\n",
        "Il clustering è una tecnica di machine learning non supervisonata, questo vuol dire che non esiste una variabile target su cui basare la bontà del modello.\n",
        "\n",
        "In statistica, il clustering è un insieme di tecniche di analisi multivariata dei dati volte alla selezione e raggruppamento di elementi omogenei in un insieme di dati. Le tecniche di clustering si basano su misure relative alla somiglianza tra gli elementi. In molti approcci questa similarità, o meglio, dissimilarità, è concepita in termini di distanza in uno spazio multidimensionale. La bontà delle analisi ottenute dagli algoritmi di clustering dipende molto dalla scelta della metrica, e quindi da come è calcolata la distanza. Gli algoritmi di clustering raggruppano gli elementi sulla base della loro distanza reciproca, e quindi l'appartenenza o meno ad un insieme dipende da quanto l'elemento preso in esame è distante dall'insieme stesso. [fonte: [Wikipedia](https://it.wikipedia.org/wiki/Clustering)]\n",
        "\n",
        "Per misurare la bontà del clustering esistono appositi indici che valutano l'omogeneità all'interno di un cluster e l'eterogeneità tra i gruppi. Con l'aiuto di queste tecniche siamo noi a decidere cosa i vari gruppi rappresentano.\n",
        "\n",
        "I problemi più frequenti per cui viene utilizzato l'approccio non supervisionato, il clustering in particolare, sono:\n",
        "* Determinare se i dati presentano relazioni significative nella forma dei concetti\n",
        "* Valutare le prestazioni di un modello di apprendimento supervisionato\n",
        "* Determinare l'insieme ideale degli attributi di input per l'apprendimento supervisionato\n",
        "* Outlier detection\n",
        "\n",
        "## Obiettivo del notebook\n",
        "\n",
        "Il notebook ha come obiettivo quello di condurre una **customer segmentation** utilizzando diversi algoritmi di clustering, in particolare:\n",
        "* K-Means\n",
        "* Clustering Gerarchico\n",
        "* DBSCAN\n",
        "* Mean Shift\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwW6QdJ4qlIg"
      },
      "source": [
        "!pip install chart_studio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGZfIlOnQzVY"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "plt.style.use('fivethirtyeight')\n",
        "import chart_studio.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAMvhhdApjdC"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "Il dataset *Mall Customer* contiene i dati di 200 clienti di un centro commerciale, contiene:\n",
        "* ID cliente\n",
        "* sesso\n",
        "* età\n",
        "* reddito annuo\n",
        "* indicatore di spesa (spending score) compreso tra 1-100.\n",
        "\n",
        "### Caricamento del dataset\n",
        "\n",
        "***Se stai usando il notebook su Colab esegui le prossime due celle, altrimenenti vai direttamente al caricamento con *read_csv* inserendo il path del tuo file *Mall_Customer.csv***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okYuc7AcsiM_"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olez5-h5sqVO"
      },
      "source": [
        "link = 'YOUR LINK'\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Mall_Customer.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0qEksHp1Ddy"
      },
      "source": [
        "df = pd.read_csv(\"Mall_Customer.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-y5h32D1tCL"
      },
      "source": [
        "### Analisi Grafica\n",
        "\n",
        "La prima cosa da fare è un'analisi grafica per avere un primo impatto del fenomeno."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-y1EZke10VU"
      },
      "source": [
        "plt.figure(1 , figsize = (25 , 8))\n",
        "n = 0 \n",
        "for x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n",
        "    n += 1\n",
        "    plt.subplot(1 , 3 , n)\n",
        "    plt.subplots_adjust(hspace =0.5 , wspace = 0.5)\n",
        "    sns.distplot(df[x] , bins = 20)\n",
        "    plt.title('Distplot of {}'.format(x))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uutBxePb3lrH"
      },
      "source": [
        "Dai grafici della distribuzione per *Reddito Annuo* e *Classi d'età* si evince che:\n",
        "* La maggior parte delle persone ha un reddito tra 50K - 75K\n",
        "* La fascia di reddito più bassa è intorno a 20K\n",
        "* La fascia d'età più presente nell'ipermercato è intorno ai 35 anni\n",
        "* I giovani frequentano più degli anziani\n",
        "* L'indice di spesa è molto simmetrico intorno ai 50K\n",
        "* Tutti i fenomeni possono essere approssimati da una distribuzione gaussiana\n",
        "\n",
        "La distribuzione per genere mostra che i clienti sono quasi equidistribuiti per genere, con leggera prevalenza delle donne.\n",
        "\n",
        "Infine il **Pairplot** ci mostra come il centro commerciale sia frequentato soprattutto da clienti con la fascia di reddito suddetta ma si rivolga anche a chi spende molto, soprattutto tra i più giovani."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZe9-Kg54lC7"
      },
      "source": [
        "labels = ['Female', 'Male']\n",
        "size = df['Genre'].value_counts()\n",
        "colors = ['lightgreen', 'orange']\n",
        "explode = [0, 0.1]\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (9, 9)\n",
        "plt.pie(size, colors = colors, explode = explode, labels = labels, shadow = True, autopct = '%.2f%%')\n",
        "plt.title('Gender', fontsize = 20)\n",
        "plt.axis('off')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "827_fj5340TU"
      },
      "source": [
        "sns.pairplot(df.drop(\"CustomerID\", axis=1), height=3.2)\n",
        "plt.title('Pairplot for the Data', fontsize= 15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R0T41Id-7ma"
      },
      "source": [
        "### K-Means\n",
        "L'algoritmo k-means (Lloyd, 1982) è una tecnica di clusterizzazione semplice ma efficace, è probabilmente la tecnica più utilizzata per fare clustering.\n",
        "Vediamo i passi dell'algoritmo:\n",
        "1. Scegliere il valore K, ovvero il numero totale di cluster da determinare\n",
        "2. Scegliere in modo casuale K osservazioni nel dataset, saranno i primi centroidi dei nostri cluster. **Questa scelta casuale influenza il numero di passi chè l'algoritmo compie per convergere.**\n",
        "3. Utilizzare una distanza (solitamente euclidea) per assegnare le restanti osservazioni al centroide più vicino\n",
        "4. Utilizzare le osservazioni in ogni cluster per calcolare la media (il baricentro) dello stesso. Ogni baricentro sarà il nuovo centroide del cluster.\n",
        "5. Se i nuovi centroidi sono identici ai precedenti il processo termina altrimenti si ripetono i punti da 3 a 5.\n",
        "\n",
        "Come abbiamo visto il numero di cluster deve essere definito a priori, per farlo si può utilizzare il **metodo di Elbow**.\n",
        "\n",
        "Questo metodo, non completamente robusto, anzi su Wikipedia è definito ambiguo, si basa sulla varianza spiegata, ovvero viene scelto il numero di cluster che dà l'incremento di varianza spiegata maggiore rispetto al precedente.\n",
        "\n",
        "<br>\n",
        "\n",
        "![elbow](https://upload.wikimedia.org/wikipedia/commons/c/cd/DataClustering_ElbowCriterion.JPG)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://en.wikipedia.org/wiki/Elbow_method_(clustering))\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Scelta del numero dei cluster\n",
        "\n",
        "Per scegliere il numero dei cluster utilizzeremo sempre il metodo di Elbow ma sull'inierzia. Questa è \"il contrario\" della varianza quindi và minimizzata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XDuCMJP6CeO"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "X = df[['Age' , 'Annual Income (k$)' ,'Spending Score (1-100)']].values\n",
        "inertia = []\n",
        "for n in range(1 , 11):\n",
        "    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n",
        "                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n",
        "    algorithm.fit(X)\n",
        "    inertia.append(algorithm.inertia_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSRXKOuDouxj"
      },
      "source": [
        "plt.figure(1 , figsize = (18 ,8))\n",
        "plt.plot(np.arange(1 , 11) , inertia , 'o')\n",
        "plt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\n",
        "plt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUPbSvEGwGW6"
      },
      "source": [
        "#### Applicazione dell'algoritmo\n",
        "\n",
        "Di seguito impostiamo i parametri dell'algoritmo su Scikit-learn:\n",
        "* n_clusters: il numero di cluster da creare\n",
        "* init: il metodo di inizializzazione. *k-means++* seleziona i punti alla prima iterazione in modo da accelerare la convergenza dell'algoritmo\n",
        "* max_iter: il numero di iterazioni prima che l'algortimo si fermi se non arrivasse a convergenza\n",
        "* tol: la tolleranza per dichiarare la convergenza del K-Means e bloccare le iterazioni\n",
        "* random_state: il seed per la replicabilità dei risultati\n",
        "* algorithm: scegliamo *elkan* perchè è più performante con i dati densi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oWRQfFSq_Cj"
      },
      "source": [
        "algorithm = KMeans(n_clusters = 6 ,init='k-means++', n_init = 10 ,max_iter=300, \n",
        "                        tol=0.0001,  random_state= 42  , algorithm='elkan')\n",
        "algorithm.fit(X)\n",
        "labels = algorithm.labels_\n",
        "#centroids = algorithm.cluster_centers_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x6SUAhsvycX"
      },
      "source": [
        "#### Grafico dei risultati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSGyJL_HrKJ4"
      },
      "source": [
        "df['label'] =  labels\n",
        "trace1 = go.Scatter3d(\n",
        "    x= df['Age'],\n",
        "    y= df['Spending Score (1-100)'],\n",
        "    z= df['Annual Income (k$)'],\n",
        "    mode='markers',\n",
        "     marker=dict(\n",
        "        color = df['label'], \n",
        "        size= 20,\n",
        "        line=dict(\n",
        "            color= df['label'],\n",
        "            width= 12\n",
        "        ),\n",
        "        opacity=0.8\n",
        "     )\n",
        ")\n",
        "data = [trace1]\n",
        "layout = go.Layout(\n",
        "    title= 'Clusters',\n",
        "    scene = dict(\n",
        "            xaxis = dict(title  = 'Age'),\n",
        "            yaxis = dict(title  = 'Spending Score'),\n",
        "            zaxis = dict(title  = 'Annual Income')\n",
        "        )\n",
        ")\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "fig.show(render=\"notebook\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2eE_HhqImXW"
      },
      "source": [
        "### Clustering Gerarchico\n",
        "\n",
        "Il clustering gerarchico è un approccio di clustering che mira a costruire una gerarchia di cluster. Le strategie per il clustering gerarchico sono tipicamente di due tipi:\n",
        "* **Agglomerativo**: alla prima iterazione ogni osservazione è un cluster e ad ogni passo vengono aggregati in gruppi sempre più numerosi secondo una distanza\n",
        "* **Divisivo**: si parte da un solo cluster e si suddividono le osservazioni ad ogni iterazione\n",
        "\n",
        "Per decidere quali cluster devono essere combinati è necessario definire una misura di similarità (o dissimilarità) tra cluster. Nella maggior parte dei metodi di clustering gerarchico si fa uso di metriche specifiche che quantificano la distanza tra coppie di elementi.\n",
        "\n",
        "A differenza del K-Means non ci serve un numero di cluster definito a priori.\n",
        "\n",
        "<br>\n",
        "\n",
        "![HClus](https://new.pharmacelera.com/wp-content/uploads/2019/08/clusters-1024x530.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://new.pharmacelera.com/science/clustering-methods-big-library-screening/)\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Applicazione dell'algoritmo\n",
        "\n",
        "Impostiamo l'algoritmo su Scikit-learn, useremo il metodo *agglomerativo*:\n",
        "* n_clusters: il numero di cluster. Può essere non definito ma bisogna dare la metrica di similarità.\n",
        "* distance_threshold: la metrica. Imposteremo a 0, quindi *full_tree*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXcp1i3dMevY"
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "AggC = AgglomerativeClustering(n_clusters = None, distance_threshold=0)\n",
        "AggC.fit(X)\n",
        "\n",
        "labels = AggC.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpVClELgM64v"
      },
      "source": [
        "#### Grafico dei risultati\n",
        "\n",
        "Per vedere i risultati del clustering gerarchico il grafico è il dendrogramma.\n",
        "\n",
        "Nell'[esempio ufficiale di scikit-learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py) troviamo una funzione per costruire un buon dendrogramma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm5CHxtJOEsf"
      },
      "source": [
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
        "                                      counts]).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-YAAGT4PTzP"
      },
      "source": [
        "Vediamo che scendendo a 4 livelli vengono formati tre cluster distinti."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebi1oIIgM_Cn"
      },
      "source": [
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "# plot the top three levels of the dendrogram\n",
        "plot_dendrogram(AggC, truncate_mode='level', p=4)\n",
        "plt.xlabel(\"Numero di osservazioni per nodo.\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sftoPYavIf2o"
      },
      "source": [
        "### DBSCAN\n",
        "\n",
        "<br>\n",
        "\n",
        "![DBSCAN](https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/DBSCAN-Illustration.svg/400px-DBSCAN-Illustration.svg.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://it.wikipedia.org/wiki/Dbscan)\n",
        "\n",
        "<br>\n",
        "\n",
        "Il **DBSCAN** è un lagoritmo basato sulla densità e connette regioni di punti con densità sufficientemente alta.\n",
        "\n",
        "Necessita di due parametri: ε (eps) e del numero minimo di punti richiesti per formare un cluster (minPts). Si comincia con un punto casuale che non è stato ancora visitato. Viene calcolato il suo ε-vicinato e se contiene un numero sufficiente di punti viene creato un nuovo cluster. Se ciò non avviene il punto viene etichettato come rumore e successivamente potrebbe essere ritrovato in un ε-vicinato sufficientemente grande riconducibile ad un punto differente entrando a far parte di un cluster.\n",
        "\n",
        "Se un punto è associato ad un cluster anche i punti del suo ε-vicinato sono parte del cluster. Conseguentemente tutti i punti trovati all'interno del suo ε-vicinato sono aggiunti al cluster, così come i loro ε-vicinati. Questo processo continua fino a quando il cluster viene completato. Il processo continua fino a quando non sono stati visitati tutti i punti.\n",
        "\n",
        "Vantaggi:\n",
        "* non richiede di conoscere il numero di cluster a priori\n",
        "* può trovare cluster di forme arbitrarie.\n",
        "* possiede la nozione di rumore\n",
        "* richiede soltanto due parametri ed è per lo più insensibile all'ordine dei punti nel database\n",
        "\n",
        "Svantaggi:\n",
        "* La qualità del clustering generato da DBSCAN dipende dalla sua misura della distanza. ***Maledizione della dimensionalità***\n",
        "* Non è in grado di classificare insiemi di dati con grandi differenze nelle densità (dati sparsi)\n",
        "\n",
        "[Fonte: DBSCAN](https://en.wikipedia.org/wiki/DBSCAN)\n",
        "\n",
        "#### Applicazione dell'algoritmo\n",
        "\n",
        "Impostiamo l'algoritmo su Scikit-learn:\n",
        "* eps: la massima distanza tra due campioni per essere considerati vicini\n",
        "* min_samples: il numero minimo di osservazioni per essere considerato un cluster core\n",
        "* algorithm: algoritmo usato dal KNN per formare i cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L-Tc_CtIiP6"
      },
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples = 10, algorithm = \"ball_tree\")\n",
        "dbscan.fit(X_scaled)\n",
        "labels = dbscan.labels_\n",
        "\n",
        "# si usa questo metodo perchè i -1 sono \"noise\" data\n",
        "print(\"Il numero di cluster è pari a: \", len(set(labels)) - (1 if -1 in labels else 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JGx7N7oIH-k"
      },
      "source": [
        "#### Grafico dei risultati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6Zv3pNvzH9d"
      },
      "source": [
        "df['label'] =  labels\n",
        "trace1 = go.Scatter3d(\n",
        "    x= df['Age'],\n",
        "    y= df['Spending Score (1-100)'],\n",
        "    z= df['Annual Income (k$)'],\n",
        "    mode='markers',\n",
        "     marker=dict(\n",
        "        color = df['label'], \n",
        "        size= 20,\n",
        "        line=dict(\n",
        "            color= df['label'],\n",
        "            width= 12\n",
        "        ),\n",
        "        opacity=0.8\n",
        "     )\n",
        ")\n",
        "data = [trace1]\n",
        "layout = go.Layout(\n",
        "    title= 'Clusters',\n",
        "    scene = dict(\n",
        "            xaxis = dict(title  = 'Age'),\n",
        "            yaxis = dict(title  = 'Spending Score'),\n",
        "            zaxis = dict(title  = 'Annual Income')\n",
        "        )\n",
        ")\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "fig.show(render=\"notebook\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5yPwaurzezD"
      },
      "source": [
        "### Mean Shift\n",
        "\n",
        "<br>\n",
        "\n",
        "![Mean Shift](https://media.geeksforgeeks.org/wp-content/uploads/20190508162515/anigif.gif)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://www.geeksforgeeks.org/ml-mean-shift-clustering/)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Mean shift** è un metodo non parametrico per la ricerca delle mode di una funzione di densità di probabilità.\n",
        "\n",
        "Introdotto nel 1975 da Fukunanga e Hostetler,[2] è equivalente all'applicazione della discesa del gradiente alla stima kernel di densità della distribuzione.\n",
        "L'algoritmo non richiede assunzioni sulla forma dei cluster e ha un singolo parametro, l'ampiezza di banda, la cui determinazione è tuttavia non banale in generale. Mean shift ha applicazioni in analisi dei cluster, image processing e computer vision.\n",
        "\n",
        "Mean shift è un algoritmo iterativo per determinare i massimi locali di una funzione di densità di probabilità a partire da un dataset di campioni, la sua complessità è alta quindi i tempi di calcolo non sono immediati, inoltre non è stata dimostrata la convergenza nel caso generale.\n",
        "\n",
        "I campi di applicazione più frequenti sono:\n",
        "\n",
        "* clustering;\n",
        "* video tracking;\n",
        "* riduzione del rumore da un segnale.\n",
        "\n",
        "\n",
        "[Fonte: Mean Shift](https://en.wikipedia.org/wiki/Mean_shift)\n",
        "\n",
        "#### Applicazione dell'algoritmo\n",
        "\n",
        "Impostiamo l'algoritmo su Scikit-learn:\n",
        "* bandwidth: l'ampiezza della banda utilizzata con RBF kernel, se non viene indicata sciki-learn utilizza la funzione *sklearn.cluster.estimate_bandwidth* per stimarla\n",
        "* bin_seeding: Default False, i punti dei kernel sono delle osservazioni, nell'esempio è impostato a True per usare la versione discretizzata dei punti e velocizzare il calcolo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXWTtRKQ2dYi"
      },
      "source": [
        "from sklearn.cluster import MeanShift, estimate_bandwidth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJaqnEM24APU"
      },
      "source": [
        "# quantile: 0.2. Default 0.5, vuol dire che viene usata la mediana di tutte le distanze a coppie\n",
        "# n_samples: il numero di campioni da utilizzare, al salire di questo trovava meno cluster\n",
        "\n",
        "bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=20, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGWcouIV4e7G"
      },
      "source": [
        "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
        "ms.fit(X)\n",
        "labels = ms.labels_\n",
        "cluster_centers = ms.cluster_centers_\n",
        "\n",
        "labels_unique = np.unique(labels)\n",
        "n_clusters_ = len(labels_unique)\n",
        "\n",
        "print(\"number of estimated clusters : %d\" % n_clusters_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE0B7L-pgNAx"
      },
      "source": [
        "### Gaussian Mixture Models\n",
        "\n",
        "<br>\n",
        "\n",
        "![GMM](https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_pdf_0011.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://scikit-learn.org/stable/modules/mixture.html)\n",
        "\n",
        "L'idea alla base di questa tecnica è che molti fenomeni in natura possono essere modellati con una distribuzione gaussiana, quindi i cluster che si possono generare da questo fenomeno non saranno altro che [misture di diverse distribuzioni gaussiane](https://en.wikipedia.org/wiki/Mixture_model#Multivariate_Gaussian_mixture_model).\n",
        "\n",
        "Per scegliere quanti cluster/misture avere un criterio utilizzato è scegliere quello che minimizza il [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion) o l'[AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion).\n",
        "\n",
        "#### Applicazione dell'algoritmo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSYMC6D-k99w"
      },
      "source": [
        "from sklearn.mixture import GaussianMixture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr-FEmxHEw5M"
      },
      "source": [
        "n_components = np.arange(1, 21)\n",
        "models = [GaussianMixture(n, covariance_type='full', random_state=42).fit(X)\n",
        "          for n in n_components]\n",
        "\n",
        "plt.plot(n_components, [m.bic(X) for m in models], label='BIC')\n",
        "plt.plot(n_components, [m.aic(X) for m in models], label='AIC')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('n_components');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzxBDeu3GmTz"
      },
      "source": [
        "Possiamo vedere che AIC ci consiglia un numero alto, intorno a 16 misture, mentre il BIC solo 5.\n",
        "\n",
        "Il BIC ci indica anche il numero che porta a generare un modello non troppo complesso, quindi bisogna mitigare tra i due valori.\n",
        "\n",
        "Ricordiamo che i GMM sono dei modelli nati in origine per stimare la densità dei dati non per il clustering, anche per questo a volte c'è molta distanza, soprattutto su dataset complessi tra i due indici."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m2BHBoymWYd"
      },
      "source": [
        "gmm = GaussianMixture(n_components=5,random_state=42)\n",
        "labels = gmm.fit_predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMFgotV9KXWr"
      },
      "source": [
        "df['label'] =  labels\n",
        "trace1 = go.Scatter3d(\n",
        "    x= df['Age'],\n",
        "    y= df['Spending Score (1-100)'],\n",
        "    z= df['Annual Income (k$)'],\n",
        "    mode='markers',\n",
        "     marker=dict(\n",
        "        color = df['label'], \n",
        "        size= 20,\n",
        "        line=dict(\n",
        "            color= df['label'],\n",
        "            width= 12\n",
        "        ),\n",
        "        opacity=0.8\n",
        "     )\n",
        ")\n",
        "data = [trace1]\n",
        "layout = go.Layout(\n",
        "    title= 'Clusters',\n",
        "    scene = dict(\n",
        "            xaxis = dict(title  = 'Age'),\n",
        "            yaxis = dict(title  = 'Spending Score'),\n",
        "            zaxis = dict(title  = 'Annual Income')\n",
        "        )\n",
        ")\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "fig.show(render=\"notebook\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPaLZiQmKnOz"
      },
      "source": [
        "#### Valutazione degli algoritmi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euKQD0DKxpVK"
      },
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "print(\"k-means: \", silhouette_score(X, algorithm.labels_))\n",
        "#print(\"H.C: \", silhouette_score(X, AggC.labels_))\n",
        "print(\"DBSCAN: \" , silhouette_score(X, dbscan.labels_))\n",
        "print(\"Mean Shift\", silhouette_score(X, ms.labels_))\n",
        "print(\"GMM\", silhouette_score(X, labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OY5w5n8wqAQ"
      },
      "source": [
        "### Link Utili\n",
        "\n",
        "[Customer Segmentation Kaggle](https://www.kaggle.com/kushal1996/customer-segmentation-k-means-analysis)\n",
        "\n",
        "[Documentazione K-Means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
        "\n",
        "[Documentazione DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n",
        "\n",
        "[k-means++](https://en.wikipedia.org/wiki/K-means%2B%2B)\n",
        "\n",
        "[Plotly](https://plot.ly/)\n",
        "\n",
        "[Maledizione della dimensionalità](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_functions)\n",
        "\n",
        "[Algoritmo KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
        "\n",
        "[Kernel Density Estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation)\n",
        "\n",
        "[Silhoutte Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)\n",
        "\n",
        "[EM Algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)."
      ]
    }
  ]
}