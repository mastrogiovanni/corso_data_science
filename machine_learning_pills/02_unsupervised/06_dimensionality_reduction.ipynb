{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_dimensionality_reduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvX1M+iXsjtw2In8cbokOf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickprock/corso_data_science/blob/devs/machine_learning_pills/02_unsupervised/06_dimensionality_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XJ5I-tKGtgc",
        "colab_type": "text"
      },
      "source": [
        "## Dimensionality Reduction\n",
        "\n",
        "Le tecniche per la riduzione della dimensionalità sono molto utili quando abbiamo molte featurese il modello non riesce atrovare pattern nei dati per questo motivo.\n",
        "\n",
        "Ne esistono diverse ma in questo notebook vedremo la più utilizzata, la Principal Component Analysis (PCA).\n",
        "\n",
        "Funziona quando ci sono relazioni lineari tra le variabili, quindi in presenza di variabili numeriche continue. **L'obiettivo è quello di ridurre ma con la minima perdita di informazione.**\n",
        "\n",
        "<br>\n",
        "\n",
        "![pca](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.stack.imgur.com%2FG7Gkv.png&f=1&nofb=1)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://stats.stackexchange.com/questions/320743/why-are-eigenvectors-the-principal-components-in-principal-component-analysis)\n",
        "\n",
        "<br>\n",
        "\n",
        "Un esempio molto semplice per spiegare la PCA è quello dei pesci. Abbiamo dei pesci e possono essere misurati per:\n",
        "* lunghezza\n",
        "* altezza\n",
        "* peso\n",
        "\n",
        "Queste tre dimensioni sono correlate tra loro, quindi possono essere riassunte in un'unica nuova dimensione:\n",
        "* stazza\n",
        "\n",
        "### Dataset\n",
        "\n",
        "Per questo esempio utilizzeremo nuovamente il dataset Iris già visto nel notebook sui decision tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dseLL66-GoEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPSWCFMpQGll",
        "colab_type": "text"
      },
      "source": [
        "Riproduciamo l'esperimento con il decision tree ma con tutte le variabili.\n",
        "\n",
        "Il dataset è semplice e l'accuratezza sarà pari ad 1.\n",
        "\n",
        "Lo scopo di questo mnotebook non è un miglioramento ma mostrare la poca perdita di informazione utilizzando la PCA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd3n4XeONtqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris = load_iris()\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFbn6b8pOP_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(train_x, train_y)\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "yhat = clf.predict(test_x)\n",
        "\n",
        "print(\"execution time: \", time.time() - start_time)\n",
        "print(\"\\n\")\n",
        "print(\"accuracy: \", accuracy_score(test_y, yhat))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oZBiYe4QY7M",
        "colab_type": "text"
      },
      "source": [
        "### PCA: scegliere il numero di componenti a priori\n",
        "\n",
        "In questo caso l'iperparametro da impostare è il numero stesso delle componenti.\n",
        "\n",
        "Vogliamo due dimensioni, vedremo come sono fatte le componenti.\n",
        "\n",
        "Notare che sul train applichiamo *fit_transform* mentre sul test solo *transform* questo vale per tutte le trasformazioni. Tra train e test ci potrebbe essere una scala di valori diversa che indurrebbe in errore lo stimatore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sGlArx0N9Af",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "train_x_pca = pca.fit_transform(train_x)\n",
        "test_x_pca = pca.transform(test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvQ4CdNnWe3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in np.unique(train_y):\n",
        "  mask = train_y == i\n",
        "  plt.scatter(train_x_pca[mask, 0], train_x_pca[mask, 1], label=i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33Tow-13P2ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf.fit(train_x_pca, train_y)\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "yhat_pca = clf.predict(test_x_pca)\n",
        "\n",
        "print(\"execution time: \", time.time() - start_time)\n",
        "print(\"\\n\")\n",
        "print(\"accuracy: \", accuracy_score(test_y, yhat_pca))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imXIAM0UReA9",
        "colab_type": "text"
      },
      "source": [
        "### PCA: scegliere la varianza spiegata\n",
        "\n",
        "Un altro iperparametro che è possibile scegliere è quanta dell'informazione nel dataset originale vogliamo mantenere.\n",
        "\n",
        "Solitamente deve essere almeno maggiore al 75% per non icorrere in errori grandi. Meglio se dal 90% in su, in questo caso una sola componente spiegherebbe quasi il 90% della variabilità quindi per averne tre usiamo il 99% come livello.\n",
        "\n",
        "Quando si usa la varianza spiegata *svd_solver* che effettua la [**Singol Value Decomposition**](https://it.wikipedia.org/wiki/Decomposizione_ai_valori_singolari) deve essere impostato a \"*full*\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeNFAL_qP-Rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.99, svd_solver=\"full\")\n",
        "train_x_pca = pca.fit_transform(train_x)\n",
        "test_x_pca = pca.transform(test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6pT3uKxSWe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(18,10))\n",
        "for i in np.unique(train_y):\n",
        "  mask = train_y == i\n",
        "  plt.subplot(3,1,1)\n",
        "  plt.scatter(train_x_pca[mask,0], train_x_pca[mask,1], label = i)\n",
        "  plt.subplot(3,1,2)  \n",
        "  plt.scatter(train_x_pca[mask,0], train_x_pca[mask,2], label = i)\n",
        "  plt.subplot(3,1,3)\n",
        "  plt.scatter(train_x_pca[mask,1], train_x_pca[mask,2], label = i)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ7-1XgySY1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf.fit(train_x_pca, train_y)\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "yhat_pca = clf.predict(test_x_pca)\n",
        "\n",
        "print(\"execution time: \", time.time() - start_time)\n",
        "print(\"\\n\")\n",
        "print(\"accuracy: \", accuracy_score(test_y, yhat_pca))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqOFYgw-YGG9",
        "colab_type": "text"
      },
      "source": [
        "### Esercizio\n",
        "\n",
        "<br>\n",
        "\n",
        "![wine](https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fdataaspirant.com%2Fwp-content%2Fuploads%2F2017%2F01%2Fwine-dataset.jpg&f=1&nofb=1)\n",
        "\n",
        "<br>\n",
        "\n",
        "[Image Credits](https://dataaspirant.com/2017/01/09/knn-implementation-r-using-caret-package/)\n",
        "\n",
        "<br>\n",
        "\n",
        "1. Creare un notebook utilizzando il dataset [Wine](http://archive.ics.uci.edu/ml/datasets/wine) e costruire un classificatore con e senza l'utilizzo della PCA.\n",
        "\n",
        "2. Valutare i risultati della classificazione\n",
        "\n",
        "3. Trovare il numero di componenti/livello di varianza spiegata ottimale\n",
        "\n",
        "4. Scatterplot del risultato\n",
        "\n",
        "**N.B. La classe [1, 2, 3] sta nella prima colonna del file.**"
      ]
    }
  ]
}